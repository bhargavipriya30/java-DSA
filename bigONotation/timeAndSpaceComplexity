Big O is a notation used to describe the "computational complexity" of an algorithm.
The computational complexity of an algorithm is split into two parts:
    1.Time complexity
    2.Space complexity.

- The time complexity of an algorithm is the amount of time the algorithm needs to run relative to the input size.
- The space complexity of an algorithm is the amount of memory used by the algorithm relative to the input size.

Calculating complexity:
Roughly, your function calculates the number of operations or amount of memory
(depending on if you're analyzing time or space complexity, respectively) your algorithm consumes relative to the input size.
Using the example from algorithm file under bigONotation (find the largest number in nums), we have a time complexity of O(n).
- The algorithm involves iterating over each element in nums, so if we define n as the length of nums, our algorithm uses approximately
n steps.
If we pass an array with a length of 10, it will perform approximately 10 steps.
If we pass an array with a length of 10,000, it will perform approximately 10,000 steps.

Rules:
1. There are a few rules when it comes to calculating complexity. First, we ignore constants.
That means O(9999999n)=O(8n)=O(n)=O(n/500) Why do we do this?
Imagine you had two algorithms. Algorithm A uses approximately, n operations and algorithm B uses approximately, 5n operations.

When n=100, algorithm A uses 100 operations and algorithm B uses 500 operations. What happens if we double n?
Then algorithm A uses 200 operations and algorithm B uses 1000 operations. As you can see, when we double the value of n,
both algorithms require double the amount of operations. If we were to 10x the value of n, then both algorithms would require 10x more operations.

Remember: the point of complexity is to analyze the algorithm as the input changes.
We don't care that algorithm B is 5x slower than algorithm A.
For both algorithms, as the input size increases, the number of operations required increases linearly.
That's what we care about. Thus, both algorithms are O(n).

2. The second rule is that we consider the complexity as the variables tend to infinity.
When we have addition/subtraction between terms of the same variable, we ignore all terms except the most powerful one.
For example, O(2n+ n^2−500n)=O(2^n). Why?
Because as n tends to infinity, 2^n becomes so large that the other two terms are effectively zero in comparison.

Let's say that we had an algorithm that required n+500 operations. It has a time complexity of O(n).
When n is small, let's say n=5, the +500 term is very significant - but we don't care about that. We need to perform the analysis as if
n is tending toward infinity, and in that scenario, the 500 is nothing.

Note:
The best complexity possible is O(1), called "constant time" or "constant space".
It means that the algorithm ALWAYS uses the same amount of resources, regardless of the input.

Note that a constant time complexity doesn't necessarily mean that an algorithm is fast
(for example, O(5000000)=O(1)), it just means that its runtime is independent of the input size.

When talking about complexity, there are normally three cases:

1. Best case scenario
2. Average case
3. Worst case scenario

Logarithmic time

A logarithm is the inverse operation to exponents. The time complexity O(logn) is called logarithmic time and is extremely fast.
A common time complexity is O(n⋅logn), which is reasonably fast for most problems and also the time complexity of efficient sorting algorithms.

- O(logn) means that somewhere in your algorithm, the input is being reduced by a percentage at every step.
A good example of this is binary search

- Space Complexity
When you initialize variables like arrays or strings, your algorithm is allocating memory.
We never count the space used by the input (it is bad practice to modify the input),
and usually don't count the space used by the output (the answer) unless an interviewer asks us to.




